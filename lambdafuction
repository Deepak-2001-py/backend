import os
import asyncio
import boto3
import json
import google.generativeai as genai
from concurrent.futures import ThreadPoolExecutor
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse,HTMLResponse
from pydantic import BaseModel
from mangum import Mangum
from io import BytesIO
import tempfile

app = FastAPI()

MAX_WORKERS = min(10, (os.cpu_count() or 1) * 2)
S3_UPLOAD_PREFIX = "uploads/"
TEXT_PREFIX = "text/"

session = boto3.Session()
s3_client = session.client('s3')
ssm_client = session.client('ssm', region_name='eu-north-1')

try:
    BUCKET_NAME = ssm_client.get_parameter(Name='pdf_bucket', WithDecryption=True)['Parameter']['Value']
    GOOGLE_API_KEY = ssm_client.get_parameter(Name='GOOGLE_API_KEY', WithDecryption=True)['Parameter']['Value']
    print(f"Configuration loaded: BUCKET_NAME={BUCKET_NAME}")
except Exception as e:
    BUCKET_NAME = os.environ.get('PDF_BUCKET')
    GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')
    print(f"Falling back to environment variables: BUCKET_NAME={BUCKET_NAME}")
    if not BUCKET_NAME or not GOOGLE_API_KEY:
        raise RuntimeError(f"Configuration error: {str(e)}")

genai.configure(api_key=GOOGLE_API_KEY)
gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')
PROMPT = "TASK - Transcribe this physics document. Output- Transcribed document"
print("Gemini AI model initialized")

class TextResponse(BaseModel):
    text: str

class BatchResponse(BaseModel):
    processed: int
    failed: int
    results: list

# File validation function
def validate_file(file: UploadFile):
    allowed_extensions = {'.pdf'}
    file_ext = os.path.splitext(file.filename)[1].lower()
    print(f"Validating file: {file.filename}, extension: {file_ext}")
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=422,
            detail=f"File type not allowed. Allowed types: {', '.join(allowed_extensions)}"
        )
    print(f"File {file.filename} validated successfully")
    return True

def process_pdf(file_data: bytes, file_name: str) -> str:
    try:
        print(f"Starting PDF processing for {file_name}")
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(file_data)
            tmp_file_path = tmp_file.name
            print(f"Temporary file created at {tmp_file_path} for {file_name}")
        
        try:
            print(f"Uploading {file_name} to Gemini API")
            sample_file = genai.upload_file(path=tmp_file_path, display_name=file_name)
        except Exception as upload_error:
            print(f"File upload to Gemini failed for {file_name}: {str(upload_error)}")
            raise Exception(f"Gemini upload failed: {str(upload_error)}")
        
        config = genai.GenerationConfig(
            max_output_tokens=8192,  # Increased for better handling of larger documents
            temperature=0.1,         # Lowered for more deterministic output
        )
        
        print(f"Generating content for {file_name} with Gemini")
        try:
            response = gemini_model.generate_content(
                [sample_file, PROMPT],
                generation_config=config
            )
        except Exception as gen_error:
            print(f"Gemini content generation failed for {file_name}: {str(gen_error)}")
            raise Exception(f"Gemini generation failed: {str(gen_error)}")
        
        print(f"Cleaning up temporary file for {file_name}")
        os.unlink(tmp_file_path)
        
        if not hasattr(response, 'text') or not response.text:
            print(f"No valid text response from Gemini for {file_name}")
            return "Failed to extract text from document"
        
        print(f"PDF processing completed for {file_name}, extracted {len(response.text)} characters")
        return response.text
    except Exception as e:
        print(f"Error processing PDF {file_name}: {str(e)}")
        raise Exception(f"Failed to process file {file_name}: {str(e)}")

@app.post("/upload-batch", response_model=BatchResponse)
async def upload_batch(files: list[UploadFile] = File(...)):
    executor = ThreadPoolExecutor(max_workers=MAX_WORKERS)
    loop = asyncio.get_event_loop()
    print(f"Starting batch upload with {len(files)} files")
    
    async def process_file(file: UploadFile):
        try:
            print(f"Processing file: {file.filename}")
            validate_file(file)
            
            print(f"Reading content of {file.filename}")
            content = await file.read()

            s3_key = f"{S3_UPLOAD_PREFIX}{file.filename}"
            print(f"Uploading {file.filename} to S3 at {s3_key}")
            s3_client.upload_fileobj(
                BytesIO(content),
                BUCKET_NAME,
                s3_key
            )
            
            print(f"Processing PDF content for {file.filename}")
            text = await loop.run_in_executor(
                executor,
                process_pdf,
                content,
                file.filename
            )
            
            
            text_key = f"{TEXT_PREFIX}{os.path.splitext(file.filename)[0]}.txt"
            print(f"Storing processed text for {file.filename} at {text_key}")
            s3_client.put_object(
                Bucket=BUCKET_NAME,
                Key=text_key,
                Body=text.encode('utf-8'),
                ContentType='text/plain'
            )
            print(f"Successfully processed {file.filename}")
            return {"filename": file.filename, "status": "success"}
        except HTTPException as e:
            print(f"Validation failed for {file.filename}: {str(e.detail)}")
            return {"filename": file.filename, "status": "error", "message": f"Validation error: {str(e.detail)}"}
        except Exception as e:
            print(f"Error processing {file.filename}: {str(e)}")
            return {"filename": file.filename, "status": "error", "message": str(e)}
        finally:
            await file.close()
            print(f"File handle closed for {file.filename}")

    results = await asyncio.gather(*[process_file(file) for file in files])
    success_count = sum(1 for r in results if r['status'] == 'success')
    print(f"Batch processing completed: {success_count} succeeded, {len(results) - success_count} failed")
    
    return {
        "processed": success_count,
        "failed": len(results) - success_count,
        "results": results
    }

@app.get("/text/{filename}", response_model=TextResponse)
async def get_text_file(filename: str):
    try:
        base_name = os.path.splitext(filename)[0]
        text_key = f"{TEXT_PREFIX}{base_name}.txt"
        print(f"Retrieving text file from S3: {text_key}")
        response = s3_client.get_object(Bucket=BUCKET_NAME, Key=text_key)
        text_content = response['Body'].read().decode('utf-8')
        print(f"Successfully retrieved text for {filename}")
        return TextResponse(text=text_content)
    except s3_client.exceptions.NoSuchKey:
        print(f"Text file not found: {text_key}")
        raise HTTPException(status_code=404, detail=f"Text file {base_name}.txt not found")
    except Exception as e:
        print(f"Error retrieving text file {filename}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error retrieving text file: {str(e)}")

@app.middleware("http")
async def error_handling_middleware(request, call_next):
    print(f"Processing request: {request.method} {request.url}")
    try:
        response = await call_next(request)
        print(f"Request completed successfully")
        return response
    except Exception as e:
        print(f"Middleware caught error: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"message": f"Internal server error: {str(e)}"}
        )
# Simple frontend for batch upload
@app.get("/", response_class=HTMLResponse)
async def upload_form():
    return """
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>PDF Batch Upload</title>
    </head>
    <body>
        <h1>Upload Multiple PDFs for Text Extraction</h1>
        <form id="uploadForm" enctype="multipart/form-data">
            <input type="file" name="files" multiple accept=".pdf" required>
            <button type="submit">Upload</button>
        </form>
        <div id="result"></div>

        <script>
            document.getElementById('uploadForm').addEventListener('submit', async (e) => {
                e.preventDefault();
                const formData = new FormData(e.target);
                const resultDiv = document.getElementById('result');
                resultDiv.innerHTML = 'Processing...';

                try {
                    const response = await fetch('/upload-batch', {
                        method: 'POST',
                        body: formData
                    });
                    const data = await response.json();
                    resultDiv.innerHTML = `
                        <h2>Results</h2>
                        <p>Processed: ${data.processed}</p>
                        <p>Failed: ${data.failed}</p>
                        <ul>${data.results.map(r => `
                            <li>${r.filename}: ${r.status} ${r.message ? ' - ' + r.message : ''}</li>
                        `).join('')}</ul>
                    `;
                } catch (error) {
                    resultDiv.innerHTML = `Error: ${error.message}`;
                }
            });
        </script>
    </body>
    </html>
    """

@app.get("/health")
def health_check():
    print("Health check requested")
    return {"status": "healthy", "version": "1.0.0"}

handler = Mangum(app, lifespan='off')
